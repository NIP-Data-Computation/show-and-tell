{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "piercel_week3_notes3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb/xeTS3Et9fOWCwsXXfCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIP-Data-Computation/show-and-tell/blob/master/piercel_week3_notes3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEwh0jEmAmwL",
        "colab_type": "text"
      },
      "source": [
        "**Author**: Pierce Lopez <br>\n",
        "**Date Created**: August 19, 2020 <br>\n",
        "**Last Updated**: August 20, 2020 <br> \n",
        "**Description**: Contains my notes on the Data Analyst lesson: _Intermediate Importing Data in Python_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbBVyq5vAm3v",
        "colab_type": "text"
      },
      "source": [
        "# Intermediate Importing Data in Python\n",
        "\n",
        "## Chapter 1: Importing Data from the Internet\n",
        "\n",
        "For this chapter, we will make use of `pandas` functions, so do not forget to import the necessary modules!\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "### Section 1: Importing flat files from the web\n",
        "1. The `urllib` Package\n",
        "* Has functions capable of fetching data on the Internet.\n",
        "* `urlopen()`: function that accepts URLs for filenames.\n",
        "\n",
        "2. How to Automate File Downloads in Python?\n",
        "\n",
        "```\n",
        "# import package\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# write contents of the url to a file\n",
        "urlretrieve(url, \"contentsarewrittenhere.csv\")\n",
        "\n",
        "# read file into a DataFrame\n",
        "df = pd.read_csv(\"contentsarewrittenhere.csv\")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: HTTP requests to import files from the web\n",
        "1. URL (Universal Resource Locator)\n",
        "* Ingredients:\n",
        "  * Protocol identifier - `http:`\n",
        "  * Resource name - `datacamp.com`\n",
        "\n",
        "2. HTTP (Hyper Text Transfer Protocol)\n",
        "* Foundation of data communication for the web.\n",
        "* Going to a website sends an HTTP request (i.e. GET request).\n",
        "* `urlretrieve()` performs a GET request.\n",
        "\n",
        "3. GET Requests Using `urllib`\n",
        "\n",
        "```\n",
        "# import package\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "# make a GET request\n",
        "request = Request(url)\n",
        "\n",
        "# send request and catch response\n",
        "response = urlopen(request)\n",
        "\n",
        "# return HTML as a string\n",
        "html = response.read()\n",
        "\n",
        "# do not forget to close the response!\n",
        "response.close()\n",
        "```\n",
        "\n",
        "4. GET Requests Using `requests`\n",
        "\n",
        "```\n",
        "# import package\n",
        "import requests\n",
        "\n",
        "# package and send request plus catch response\n",
        "r = requests.get(url)\n",
        "\n",
        "# return HTML as a string\n",
        "text = r.text\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: Scraping the web in Python\n",
        "1. BeautifulSoup\n",
        "* Parses and extracts structured data from HTML\n",
        "\n",
        "```\n",
        "# import package\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# package and send request plus catch response\n",
        "r = requests.get(url)\n",
        "\n",
        "# return HTML as a string\n",
        "html_doc = r.text\n",
        "\n",
        "# Beautify!\n",
        "soup = BeautifulSoup(html_doc)\n",
        "```\n",
        "\n",
        "2. Exploring BeautifulSoup\n",
        "* `soup.title` gets title\n",
        "* `soup.get_text()` gets text\n",
        "* `soup.find_all()` can extract URLs of all hyperlinks\n",
        "\n",
        "```\n",
        "# extract URLs of all hyperlinks\n",
        "for link in soup.find_all('a'):\n",
        "  print(link.get('href'))\n",
        "```\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77rSvNprAnX3",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 2: Interacting with APIs to Import Data From the Web\n",
        "\n",
        "### Section 1: Introduction to APIs and JSONs\n",
        "1. APIs (Application Programming Interfaces)\n",
        "* A set of protocols and routines for building and interacting with software applications.\n",
        "\n",
        "2. JSONs (JavaScript Object Notations)\n",
        "* Dictionaries in Python!\n",
        "\n",
        "3. Loading JSONs in Python\n",
        "\n",
        "```\n",
        "# import package\n",
        "import json\n",
        "\n",
        "# read by context manager\n",
        "with open('file.json', 'r') as json_file:\n",
        "  json_data = json.load(json_file)\n",
        "\n",
        "# display key-value pairs\n",
        "for key, value in json_data.items():\n",
        "  print(key + \":\", + value)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: APIs and interacting with the world wide web\n",
        "1. More Information About APIs\n",
        "* Code that allows software programs to communicate with each other.\n",
        "\n",
        "2. Connecting to an API in Python\n",
        "\n",
        "```\n",
        "# import package\n",
        "import requests\n",
        "\n",
        "# set url\n",
        "url = 'http://www.omdbapi.com/?t=hackers'\n",
        "# package and send request plus catch response\n",
        "r = requests.get(url)\n",
        "\n",
        "# return a dictionary (JSON data)\n",
        "json_data = r.json()\n",
        "\n",
        "# display key-value pairs\n",
        "for key, value in json_data.items():\n",
        "  print(key + \":\", + value)\n",
        "```\n",
        "\n",
        "3. What URL Thooooose?\n",
        "* `http` - making an HTTP request.\n",
        "* `www.omdbapi.com` - querying the OMDB API.\n",
        "* `?t=hackers` - query string that returns data for a movie title `t` _Hackers_ .\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPU9O8rZA8q_",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 3: Diving Deep into the Twitter API\n",
        "\n",
        "### Section 1: The Twitter API and authentication\n",
        "1. Twitter Has a Number of APIs\n",
        "* REST (Representational State Transfer) APIs - allows users to read and write Twitter data.\n",
        "* Streaming API - monitor/process Tweets in real-time.\n",
        "* Firehose API - access all public statuses.\n",
        "\n",
        "**Note:** Tweets are returned as JSONs.\n",
        "\n",
        "2. An Example of Using `tweepy` to Stream Tweets\n",
        "\n",
        "```\n",
        "# import packages\n",
        "import tweepy, json\n",
        "\n",
        "# add keys and access tokens\n",
        "access_token = \"secret\"\n",
        "access_token_secret = \"secret\"\n",
        "consumer_key = \"secret\" \n",
        "consumer_key_secret = \"secret\"\n",
        "\n",
        "# authentication handling\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# initialize stream listener\n",
        "l = MyStreamListener()\n",
        "\n",
        "# create stream object with authentication\n",
        "stream = tweepy.Stream(auth, l)\n",
        "\n",
        "# filter Twitter streams to track data by keywords:\n",
        "stream.filter(track = ['clinton', 'trump', 'sanders', 'cruz'])\n",
        "```"
      ]
    }
  ]
}