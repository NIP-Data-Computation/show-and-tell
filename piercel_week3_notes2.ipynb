{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "piercel_week3_notes2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2PhhT205G9BOUDIVp+huf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIP-Data-Computation/show-and-tell/blob/master/piercel_week3_notes2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qi-TF-uuBBq",
        "colab_type": "text"
      },
      "source": [
        "**Author**: Pierce Lopez <br>\n",
        "**Date Created**: August 19, 2020 <br>\n",
        "**Last Updated**: August 19, 2020 <br> \n",
        "**Description**: Contains my notes on the Data Analyst lesson: _Introduction to Importing Data in Python_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YB9jmFvQ_w",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Importing Data in Python\n",
        "\n",
        "## Chapter 1: Introduction and Flat Files\n",
        "\n",
        "### Section 1: Welcome to the course!\n",
        "1. Reading a Text File\n",
        "\n",
        "```\n",
        "# have a filename\n",
        "filename =\n",
        "\n",
        "# open a connection to the file\n",
        "# 'r' = read\n",
        "file = open(filename, mode = 'r')\n",
        "\n",
        "# read file\n",
        "text = file.read()\n",
        "\n",
        "# don't forget to close the connection!\n",
        "file.close()\n",
        "```\n",
        "\n",
        "2. Writing on a Text File\n",
        "\n",
        "```\n",
        "# have a filename\n",
        "filename =\n",
        "\n",
        "# open a connection to the file\n",
        "# 'w' = write\n",
        "file = open(filename, mode = 'w')\n",
        "\n",
        "# don't forget to close the connection!\n",
        "file.close()\n",
        "```\n",
        "\n",
        "3. Using `with` as an Alternative to `.close()`\n",
        "\n",
        "```\n",
        "# using with (context manager)\n",
        "\n",
        "with open(filename, mode) as file:\n",
        "  print(file.read())\n",
        "```\n",
        "\n",
        "**Note:** Once removing the indentation, we get out of the `with` clause and the connection automatically closes.\n",
        "\n",
        "**Additional insights:**\n",
        "1. We can print the first few lines of the text file by using `file.readline()` several times.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: The importance of flat files in data science\n",
        "1. Flat Files\n",
        "* Basic text files containing records (i.e. tabled data without structured relationships).\n",
        "* Consists of records (rows of fields an attributes).\n",
        "* Can have headers (commonly the first row of the flat file), describing the contents of the flat file.\n",
        "\n",
        "2. Examples of Flat Files\n",
        "* .csv - Comma-Separated Values\n",
        "* .txt\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: Importing flat files using `numpy`\n",
        "1. Why NumPy?\n",
        "* NumPy's a standard for numerical data storage.\n",
        "\n",
        "2. Importing Flat Files Using `numpy`\n",
        "\n",
        "```\n",
        "# import module\n",
        "import numpy as np\n",
        "\n",
        "# load the file\n",
        "data = np.loadtxt(filename, delimiter = ',')\n",
        "```\n",
        "\n",
        "**Note:** Delimiters are separators of values in file (i.e. commas, periods, `Tab`:'\\t') that we want to specify.\n",
        "\n",
        "3. Customizing Your NumPy Import\n",
        "* `skiprows`: neglects rows indicated when loading the file.\n",
        "* `usedols`: loads specified columns only\n",
        "* `dtype = str`: changes the data type of the values loaded into strings\n",
        "\n",
        "**Note:** `skiprows` starts with `1` while `usecols` starts with `0` for their indices.\n",
        "\n",
        "**Additional insights:**\n",
        "1. `np.genfromtxt(dtype = None)` can handle files with different data types on different columns.\n",
        "2. The `names = True` argument specifies that the file that we are loading has a header.\n",
        "3. `np.recfromcsv(file)` works similarly with `np.genfromtxt(file, delimiter = ',', names = true, dtype = None)`.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 4: Importing flat files using `pandas`\n",
        "1. Importing Using `pandas`\n",
        "\n",
        "```\n",
        "# import modules\n",
        "import pandas as pd\n",
        "\n",
        "# read file into DataFrame\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# convert DataFrame into NumPy array\n",
        "df_array_form = df.values\n",
        "```\n",
        "\n",
        "**Additional insights:** <br>\n",
        "Some `read_csv()` parameters:\n",
        "* `sep`: `pandas` version of `delimiter`.\n",
        "* `nrows`: specifies first n rows of data to be read.\n",
        "* `header`: specifies if there is a header.\n",
        "* `comment`: specifies a string/character to which comments follow.\n",
        "* `na_values`: specifies a list of strings considered to be `NA` or `NaN` (missing values).\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUNvyJiuvRhY",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 2: Importing Data from Other File Types\n",
        "For this chapter, we will make use of `pandas` functions, so do not forget to import the necessary modules!\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "### Section 1: Introduction to other file types\n",
        "1. Pickled Files\n",
        "* File type native to Python.\n",
        "* Are serialized (objects are converted into bytes).\n",
        "\n",
        "```\n",
        "# import module\n",
        "import pickle\n",
        "\n",
        "# open file (rd - read-only and binary)\n",
        "with open(\"filename.pkl\", \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "# display data\n",
        "print(data)\n",
        "```\n",
        "\n",
        "2. Import Excel Spreadsheets\n",
        "\n",
        "```\n",
        "# import module\n",
        "import pandas as pd\n",
        "\n",
        "# read Excel file\n",
        "data = pd.ExcelFile(file)\n",
        "\n",
        "# display sheet names\n",
        "print(data.sheet_names)\n",
        "\n",
        "# load specific sheet\n",
        "df = data.parse(\"sheet_name\")\n",
        "df = data.parse(sheet_index)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Importing SAS/Stata files using `pandas`\n",
        "\n",
        "SAS: Statistical Analysis System (business analytics and biostatistics) <br>\n",
        "Stata: \"Statistics\" + \"data\" (academic social sciences research)\n",
        "\n",
        "1. Importing SAS Files\n",
        "\n",
        "```\n",
        "# import necessary modules\n",
        "from sas7bdat import SAS7BDAT\n",
        "\n",
        "# read file\n",
        "with SAS7BDAT(\"filename.sas7bdat\") as file:\n",
        "  df_sas = file.to_data_frame()\n",
        "```\n",
        "\n",
        "2. Importing Stata Files\n",
        "\n",
        "```\n",
        "# read file\n",
        "data = pd.read_stata(\"filename.dta\")\n",
        "```\n",
        "\n",
        "### Section 3: Importing HDF5 files\n",
        "\n",
        "HDF5: Hierarchical Data Format version 5 which is a standard for storing large quantities of numerical data (up to exabyte storage capacity (i can't even imagine how big this is lol)).\n",
        "\n",
        "1. Importing HDF5 Files\n",
        "\n",
        "```\n",
        "# import necessary modules\n",
        "import h5py\n",
        "\n",
        "# read file\n",
        "data = h5py.File(filename, 'r')\n",
        "\n",
        "# display data\n",
        "print(data)\n",
        "```\n",
        "\n",
        "2. The Structure of HDF5 Files\n",
        "\n",
        "HDF5's hierarchical data file structures can be accessed like keys in a dictionary.\n",
        "\n",
        "```\n",
        "# access HDF5 structure\n",
        "for key in data.keys():\n",
        "  print(key)\n",
        "```\n",
        "\n",
        "Each key is a HDF5 directory which can also contain sub-keys!\n",
        "\n",
        "```\n",
        "# access HDF5 \"sub-directory\"\n",
        "for subkey in data['group_name'].keys():\n",
        "  print(subkey)\n",
        "```\n",
        "\n",
        "Accessing values within a sub-directory is like accessing a multi-level dictionary!\n",
        "\n",
        "```\n",
        "# access HDF5 \"sub-directory\" values\n",
        "print(data['group_name']['subgroup_name'].value)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 4: Importing MATLAB files\n",
        "\n",
        "1. MATLAB\n",
        "* Short for \"Matrix Laboratory\".\n",
        "* Data saved as .mat files.\n",
        "\n",
        "2. SciPy to the Rescue!\n",
        "* `scipy.io.loadmat()` - read .mat files.\n",
        "* `scipy.io.savemat()` - write .mat files.\n",
        "\n",
        "3. What is a .mat File?\n",
        "* A .mat file contains all objects (variables, arrays, vectors, etc.) stored in the MATLAB workspace.\n",
        "\n",
        "4. Importing a .mat File\n",
        "\n",
        "```\n",
        "# import module\n",
        "import scipy.io\n",
        "\n",
        "# read .mat file\n",
        "mat = scipy.io.loadmat(filename)\n",
        "\n",
        "# display data\n",
        "print(mat)\n",
        "```\n",
        "\n",
        "**Note:** .mat files are dictionaries, where:\n",
        "* keys = MATLAB variable names, and\n",
        "* values = objects assigneed to those variables.\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfJZULmvvRo_",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 3: Working with Relational Databases in Python\n",
        "\n",
        "### Section 1: Introduction to relational databases\n",
        "1. What is a Relational Database?\n",
        "* A collection of data (in the form of _tables_) with defined relationships between them. These relationships are formed through links (IDs).\n",
        "\n",
        "**Note:** These _tables_ are the same as DataFrames!\n",
        "\n",
        "2. Relational Model\n",
        "* A relational model consists of the following:\n",
        "  * Tables that hold entities.\n",
        "  * Rows that hold a unique instance of an entity type.\n",
        "  * Columns that hold attributes.\n",
        "  * IDs that make an entity unique and are used to link tables to each other.\n",
        "\n",
        "3. Relational Database Management Systems\n",
        "* SQL: \"Structured Query Language\"\n",
        "* Some examples:\n",
        "  * PostgreSQL\n",
        "  * MySQL\n",
        "  * SQLite\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Creating a database engine in Python\n",
        "0. Prelude\n",
        "* SQLite databases are going to be used.\n",
        "* The `sqlalchemy` package is going to be used.\n",
        "\n",
        "1. Creating a Database Engine\n",
        "\n",
        "```\n",
        "# import\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# create database engine\n",
        "engine = create_engine('sqlite:///Database.sqlite')\n",
        "```\n",
        "\n",
        "2. Getting Table Names\n",
        "\n",
        "```\n",
        "# import\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# create database engine\n",
        "engine = create_engine('sqlite:///Database.sqlite')\n",
        "\n",
        "# display table names\n",
        "table_names = engine.table_names()\n",
        "print(table_names)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: Querying relational databases in Python\n",
        "1. Basic SQL Query\n",
        "* `SELECT * FROM Table_Name`: returns all columns of all rows of the table.\n",
        "\n",
        "2. Workflow of SQL Querying\n",
        "* Import packages and functions.\n",
        "* Create engine.\n",
        "* Conenct to engine.\n",
        "* Query the database.\n",
        "* Save query results to a DataFrame.\n",
        "* Set the DataFrame column names. (optional)\n",
        "* Close connection.\n",
        "\n",
        "```\n",
        "# import\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# create engine\n",
        "engine = create_engine('sqlite:///Database.sqlite')\n",
        "\n",
        "# connect to engine\n",
        "con = engine.connect()\n",
        "\n",
        "# query the database\n",
        "rs = con.execute(\"SELECT * FROM Table_Name\")\n",
        "\n",
        "# save query results to a DataFrame\n",
        "df = pd.DataFrame(rs.fetchall())\n",
        "\n",
        "# set DataFrame columns\n",
        "df.columns = rs.keys()\n",
        "\n",
        "# close connection\n",
        "con.close()\n",
        "```\n",
        "\n",
        "3. Using the Context Manager\n",
        "\n",
        "```\n",
        "# import\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# create engine\n",
        "engine = create_engine('sqlite:///Database.sqlite')\n",
        "\n",
        "with engine.connect() as con:\n",
        "  # query the database\n",
        "  rs = con.execute(\"SELECT Column_Name1, Column_Name2, ShipName FROM Table_Name\")\n",
        "\n",
        "  # save query results to a DataFrame\n",
        "  df = pd.DataFrame(rs.fetchmany(size = 5)) # only 5 rows instead of all rows\n",
        "\n",
        "  # set DataFrame columns\n",
        "  df.columns = rs.keys()\n",
        "```\n",
        "\n",
        "**Additional insights:**\n",
        "* SQL queries can have logical filters by including a `WHERE` statement.\n",
        "* SQL queries can have a sorting mechanism by including an `ORDER BY Table_Name` statement.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 4: Querying relational databases directly with `pandas`\n",
        "1. The `pandas` Way to Query\n",
        "\n",
        "```\n",
        "# one-liner querying!\n",
        "df = pd.read_sql_query(\"SELECT * FROM Table_Name\", engine)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 5: Advanced querying: exploiting table relationships\n",
        "1. Joining Tables (i.e. Inner Join) in Python Using `pandas`\n",
        "\n",
        "```\n",
        "# import modules\n",
        "from sqlalchemy import sqlite\n",
        "import pandas as pd\n",
        "\n",
        "# create engine\n",
        "engine = create_engine('sqlite:///Database.sqlite')\n",
        "\n",
        "# create DataFrame from SQL query results\n",
        "df = pd.read_sql_query(\"SELECT ColumnsFromFirstTable, ColumnsFromSecondTable FROM FirstTable_Name INNER JOIN SecondTable_Name on FirstTable.CommonColumn_Name = SecondTable.CommonColumn_Name, engine)\n",
        "\n",
        "# display results\n",
        "print(df)\n",
        "```"
      ]
    }
  ]
}