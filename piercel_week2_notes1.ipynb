{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "piercel_week2_notes1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRA/yL9EETDgA0p7DI8HzD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIP-Data-Computation/show-and-tell/blob/master/piercel_week2_notes1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxVblUmSaxav",
        "colab_type": "text"
      },
      "source": [
        "**Author**: Pierce Lopez <br>\n",
        "**Date Created**: August 10, 2020 <br>\n",
        "**Last Updated**: August 11, 2020 <br> \n",
        "**Description**: Contains my notes on the Data Analyst lesson: _Data Manipulation with pandas_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xs_V_HbMLz",
        "colab_type": "text"
      },
      "source": [
        "# Data Manipulation with pandas\n",
        "For this chapter, we will make use of the `pandas`, `NumPy`, and `matplotlib.pyplot` functions so do not forget to import the necessary modules!\n",
        "\n",
        "```\n",
        "# import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "## Chapter 1: Transforming Data\n",
        "\n",
        "### Section 1: Introducing DataFrames\n",
        "\n",
        "Starting with a recap:\n",
        "\n",
        "1. pandas is a high-level data analysis and visualization tool built-on the NumPy and Matplotlib modules.\n",
        "2. Ways to explore a DataFrame:\n",
        "  * `dataframe.head()` - displays first few rows\n",
        "  * `dataframe.info()` - displays DataFrame information (column names, column data types, missing values)\n",
        "  * `dataframe.describe()` - displays summary statistics for numeric columns\n",
        "  \n",
        "  **DataFrame Attributes**\n",
        "  * `dataframe.shape` - displays DataFrame dimensions (ordered pair)\n",
        "  * `dataframe.values` - displays row values in a 2D NumPy array\n",
        "  * `dataframe.columns` - displays column names\n",
        "  * `dataframe.index` - displays row names/indices\n",
        "\n",
        "**Note:** That last two attributes are Index Values, which will be discussed in a nother chapter!\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Sorting and subsetting\n",
        "\n",
        "1. Sorting\n",
        "  `dataframe.sort_values([\"col_name1\", \"col_name2\"])` \n",
        "  * sorts `col_name1` values then sorts `col_name2` values in ascending order\n",
        "  * setting `ascending = False` will sort in a descending fashion\n",
        "  * **Note:** `ascending = [list_of_boolean_values]` to set manner of sorting per every individual column.\n",
        "\n",
        "2. Subsetting columns (recap)\n",
        "  * `dataframe[\"col_name\"]` - for single columns\n",
        "  * `dataframe[[\"col_name1\", \"col_name2\"]]` - for multiple columns\n",
        "    * **Recall:** Double square brackets are used to keep the DataFrame data structure!\n",
        "\n",
        "3. Subsetting rows by comparison (recap)\n",
        "  * `filter = dataframe[\"col_name\"] > 50` - a logical filter\n",
        "  * `dataframe[filter]` - passing the logical filter\n",
        "    * **Note:** Multiple filters can be passed simultaneously using logical operators.\n",
        "\n",
        "```\n",
        "# pass multiple filters at once\n",
        "dataframe[(filter1) & (filter2)]\n",
        "```\n",
        "\n",
        "4. Subsetting rows using the `isin()` method.\n",
        "  * `filter = dataframe[\"col_name\"].isin([\"val1\", \"val2\"])` - a filter\n",
        "  * `dataframe[filter]` - passing the filter\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: New columns\n",
        "\n",
        "Adding a new column\n",
        "\n",
        "  * `dataframe[\"kilometer\"] = dataframe[\"meter\"] * 1000`\n",
        "  * `dataframe[\"sq_km\"] = dataframe[\"kilometer\"] ** 2`\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJw2ivXglcJA",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 2: Aggregating Data\n",
        "\n",
        "### Section 1: Summary statistics\n",
        "\n",
        "1. Common summary statistics:\n",
        "\n",
        "  * `.mean()` <br>\n",
        "  * `.median()` <br>\n",
        "  * `.mode()` <br>\n",
        "  * `.min()` - minimum <br>\n",
        "  * `.max()` - maximum <br>\n",
        "  * `.var()` - variance <br>\n",
        "  * `.std()` - standard deviation <br>\n",
        "  * `.sum()` <br>\n",
        "  * `.quantile()`\n",
        "\n",
        "**Note:** these can work on multiple columns!\n",
        "\n",
        "```\n",
        "mean = dataframe[\"col_name1\", \"col_name2\"].mean()\n",
        "``` \n",
        "<br>\n",
        "\n",
        "2. The `.agg()` method\n",
        "  * Allows us to solve for customized statistical values. These customized statistical values can be made by defining functions.\n",
        "\n",
        "```\n",
        "# calculate square of mean\n",
        "def sq_mean(column) :\n",
        "  return column.mean() ** 2\n",
        "\n",
        "dataframe[\"col_name1\"].agg(sq_mean)\n",
        "```\n",
        "\n",
        "**Note:** `.agg()` can be used on multiple columns and it can also be used to get multiple customized statistical values!\n",
        "\n",
        "```\n",
        "# calculate square of mean\n",
        "def sq_mean(column) :\n",
        "  return column.mean() ** 2\n",
        "\n",
        "# calculate cube of mean\n",
        "def cb_mean(column) :\n",
        "  return column.mean() ** 3\n",
        "dataframe[[\"col_name1\", \"col_name2\"]].agg([sq_mean, cb_mean])\n",
        "```\n",
        "\n",
        "2. Cumulative statistics\n",
        "\n",
        "  * `.cumsum()` - cumulative sum <br>\n",
        "  * `.cummax()` - cumulative maximum <br>\n",
        "  * `.cummin()` - cumulative minimum <br>\n",
        "  * `.cumprod()` - cumulative product\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Counting\n",
        "\n",
        "1. Dropping duplicates (to avoid miscounts)\n",
        "```\n",
        "unique = dataframe.drop_duplicates(subset = \"col_name\")`\n",
        "```\n",
        "\n",
        "**Note:** We can have two values of the same name but actually referring to two different things. In this case, we will use another subset that differentiates the two of them!\n",
        "\n",
        "```\n",
        "unique_corrected = dataframe.drop_duplicates(subset = [\"col_name1\", \"col_name2\"])\n",
        "```\n",
        "\n",
        "2. Counting\n",
        "\n",
        "```\n",
        "# counting \n",
        "unique_corrected[\"col_name\"].value_counts()\n",
        "\n",
        "# counting and sorting\n",
        "unique_corrected[\"col_name\"].value_counts(sort = True)\n",
        "\n",
        "# counting, sorting, and proportionalizing\n",
        "unique_corrected[\"col_name\"].value_counts(sort = True, normalize = True)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: Grouped summary statistics\n",
        "\n",
        "We can take summary statistics by group using the `groupby()` function to avoid repetitive typing!\n",
        "\n",
        "```\n",
        "# get group statistics\n",
        "dataframe.groupby(\"column_you_want_to_group\")[\"data_column_you want_to_get_statistics_on\"].mean()\n",
        "\n",
        "# get multiple group statistics\n",
        "dataframe.groupby(\"column_you_want_to_group\")[\"data_column_you want_to_get_statistics_on\"].agg([mean, min, max])\n",
        "```\n",
        "\n",
        "**Note:** We can also group using multiple variables!\n",
        "```\n",
        "# get multi-group statistics\n",
        "dataframe.groupby([\"column_you_want_to_group1\", \"column_you_want_to_group2\"]))[\"data_column_you want_to_get_statistics_on\"].mean()\n",
        "\n",
        "# get multi-group multi-statistics\n",
        "dataframe.groupby([\"column_you_want_to_group1\", \"column_you_want_to_group2\"]))[[\"data_column_you want_to_get_statistics_on1\", \"data_column_you want_to_get_statistics_on2\"]].mean()\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 4: Pivot tables\n",
        "\n",
        "Pivot tables are similar to the `groupby()` function.\n",
        "\n",
        "```\n",
        "# the two lines (groupby vs pivot table) are similar\n",
        "dataframe.groupby(\"column_you_want_to_group\")[\"data_column_you want_to_get_statistics_on\"].mean()\n",
        "\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group\")\n",
        "```\n",
        "\n",
        "**Note:** We can change the output statistics we want using the `aggfunc` argument.\n",
        "\n",
        "```\n",
        "# change output statistics\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group\", aggfunc = np.median)\n",
        "\n",
        "# multiple output statistics\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group\", aggfunc = [np.median, np.mean])\n",
        "```\n",
        "\n",
        "We can also use `pivot_table` on two variables:\n",
        "\n",
        "```\n",
        "# the two lines (groupby vs pivot table) are similar\n",
        "dataframe.groupby([\"column_you_want_to_group1\", \"column_you_want_to_group2\"]))[\"data_column_you want_to_get_statistics_on\"].mean()\n",
        "\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group1\", columns = \"column_you_want_to_group2\")\n",
        "```\n",
        "\n",
        "**Note:** The outputs of the two lines of code above are the same but presented in different ways. The latter produces a more expanded version with lots of `NA` values to signify _missing data_. We can change those `NA` values (with 0, for example) using the `fill_value` argument.\n",
        "\n",
        "```\n",
        "# replace NA values with 0\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group1\", columns = \"column_you_want_to_group2\", fill_value = 0)\n",
        "```\n",
        "\n",
        "We can also add a `margins = True` argument to add a new row and column which specify the mean for each column and row **NOT INCLUDING** missing values.\n",
        "\n",
        "```\n",
        "# get mean for each row and column of the pivot table\n",
        "dataframe.pivot_table(values = \"data_column_you want_to_get_statistics_on\", index = \"column_you_want_to_group1\", columns = \"column_you_want_to_group2\", fill_value = 0, margin = True)\n",
        "```\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcAMbGKN6UEO",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 3: Slicing and Indexing\n",
        "\n",
        "### Section 1: Explicit indexes\n",
        "\n",
        "1. Setting a column as indices (row labels) <br>\n",
        "`dataframe.set_index(\"col_name\")`\n",
        "\n",
        "2. Removing an index <br>\n",
        "`dataframe.reset_index()`\n",
        "\n",
        "3. Dropping an index <br>\n",
        "`dataframe.reset_index(drop = True)`\n",
        "\n",
        "  * **Note:** This completely removes the index column from the DataFrame as well, so be careful!\n",
        "\n",
        "4. Using `.loc` to subset indexes <br>\n",
        "`dataframe.loc[\"index_name\"]`\n",
        "\n",
        "5. Multi-level indexing <br>\n",
        "`dataframe.set_index([\"col_name1\", \"col_name2])`\n",
        "  * Subsetting the outer level of a multi-level index (by a list):\n",
        "    * `dataframe.loc[[\"outer_index_name1\", \"outer_index_name2\"]]`\n",
        "  * Subsetting the inner level of a multi-level index (by ordered pairs in list)\n",
        "    * `dataframe.loc[[(\"outer_index_name1\", \"inner_index_name1\"), (\"outer_index_name2\", \"inner_index_name2\")]]`\n",
        "\n",
        "6. Sorting index values <br>\n",
        "  * Single-level index:\n",
        "    * `dataframe.sort_index()`\n",
        "  * Multi-level index: levels and manner of ordering can be changed\n",
        "    * `dataframe.sort_index(level = [], ascending = [])`\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Slicing and subsetting with `.loc` and `.iloc`\n",
        "\n",
        "This section shows a recap of Chapter 2 (Section 4) of the previous lesson (Intermediate Python). \n",
        "\n",
        "<br>\n",
        "\n",
        "1. Sort the index before you slice! <br>\n",
        "`dataframe.sort_index(level = [], ascending = [])`\n",
        "\n",
        "2. Slicing correctly!\n",
        "  * Outer-level index:\n",
        "    * `dataframe.loc[\"outer_level_index1\":\"outer_level_index2\"]`\n",
        "  * Multi-level index:\n",
        "    * `dataframe.loc[(\"outer_level_index1\",\"inner_level_index1\"):(\"outer_level_index2\",\"inner_level_index2\")]`\n",
        "    * **Note:** We don't need extra square brackets because we used the \":\" for calling!\n",
        "    \n",
        "<br>\n",
        "\n",
        "### Section 3: Working with pivot tables\n",
        "\n",
        "This section shows a recap of Chapter 2 (Section 4) of the current lesson (Data Manipulation with pandas). \n",
        "\n",
        "<br>\n",
        "\n",
        "**Additional insights:** \n",
        "\n",
        "1. We can get summary statistics across rows or columns of a pivot table using the `axis` argument\n",
        "\n",
        "```\n",
        "# mean across index values\n",
        "pivottable.mean(axis = \"index\")\n",
        "\n",
        "# mean across column values\n",
        "pivottable.mean(axis = \"columns\")\n",
        "```\n",
        "\n",
        "2. We can access the date components using the `.dt.year`, `dt.month`, and `dt.day` DataFrame attributes!\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQAGZ9QZcgFA",
        "colab_type": "text"
      },
      "source": [
        "## Chapter 4: Creating and Visualizing DataFrames\n",
        "\n",
        "### Section 1: Visualizing your data\n",
        "\n",
        "This section shows a recap of Chapter 4 of a previous lesson (Introduction to Data Science in Python).\n",
        "\n",
        "**Additional insights:** \n",
        "\n",
        "1. Instead of using `plt.hist(dataframe[\"col_name\"])`, we can opt to do `dataframe[\"col_name\"].hist()` to maybe lessen code.\n",
        "2. We can also use the `kind` argument of `plt.plot()` to change the type of plot we want to use.\n",
        "3. We can rotate the axis labels using the `rot` argument.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 2: Missing values\n",
        "\n",
        "Missing values are indicated with `NaN`, short for _not a number_.\n",
        "* `dataframe.isna()` detects elements within the dataframe that are missing through boolean values.\n",
        "* `dataframe.isna().any()` detects columns that contain missing values through boolean values.\n",
        "* `dataframe.isna().sum()` counts the number of missing values per column.\n",
        "  * We can also plot this to visualize the frequency of missing valeus per column.\n",
        "* `dataframe.dropna()` removes rows that contain missing values.\n",
        "* `dataframe.fillna(0)` replaces missing values with, in this case, 0.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 3: Creating DataFrames\n",
        "\n",
        "This section shows a recap of Chapter 2 (Sections 1-3) of the previous lesson (Intermediate Python).\n",
        "\n",
        "**Additional insights:**\n",
        "\n",
        "1. List of dictionaries constructs row-by-row values of a DataFrame.\n",
        "```\n",
        "# a list of dictionaries (keys become the column names)\n",
        "list_of_dicts = [{key-value pairs that fill the first row},{key-value pairs that fill the second row}]\n",
        "```\n",
        "2. Dictionary of lists constructs column-by-column valeus of a DataFrame.\n",
        "```\n",
        "# a dictionary of lists\n",
        "dict_of_lists = {col_name1:[list of column1 values],col_name2:[list of column2 values]}\n",
        "```\n",
        "3. Create the DataFrame by using either the list or dictionary.\n",
        "```\n",
        "# convert to DataFrame\n",
        "pd.DataFrame(list_of_dicts)\n",
        "pd.DataFrame(dict_of_lists)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Section 4: Reading and writing CSVs\n",
        "\n",
        "1. Reading\n",
        "\n",
        "`pd.read_csv(filename)`\n",
        "\n",
        "2. Writing\n",
        "\n",
        "`dataframe.to_csv(filename)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHAYPkb7q6FQ",
        "colab_type": "text"
      },
      "source": [
        "Tasks from this lesson (self-assessment)\n",
        "\n",
        "1. Transforming Data\n",
        "  * Introducing DataFrames\n",
        "    * Did I use different DataFrame attributes?\n",
        "  * Sorting and Subsetting\n",
        "    * Did I sort values of data within columns?\n",
        "  * New Columns\n",
        "    * Did I append new columns to a DataFrame?\n",
        "\n",
        "2. Aggregating Data\n",
        "  * Summary Statistics\n",
        "    * Did I obtain some summary statistics from the Customs dataset?\n",
        "    * Did I use the `agg()` function to obtain multiple statistics and/or customized statistics?\n",
        "  * Counting\n",
        "    * Did I drop duplicates, if there are any?\n",
        "    * Did I use the `value_counts()` function?\n",
        "  * Grouped Summary Statistics\n",
        "    * Did I use the `groupby()` function to collect similar groups?\n",
        "  * Pivot Tables\n",
        "    * Did I create pivot tables?\n",
        "\n",
        "3. Slicing and Indexing\n",
        "  * Explicit Indexes\n",
        "    * Did I set a column as an index?\n",
        "    * Did I create a multi-level indexed DataFrame?\n",
        "  * Slicing and Subsetting with `loc[]` and `iloc[]`\n",
        "    * Did I subset/slice a DataFrame using any of these methods?\n",
        "  * Working with Pivot Tables\n",
        "    * Did I use the `axis` argument to get a statistic across a row/column?\n",
        "\n",
        "4. Creating and Visualizing DataFrames\n",
        "  * Visualizing Your Data\n",
        "    * Did I use line plots?\n",
        "    * Did I use scatter plots?\n",
        "    * Did I use bar plots?\n",
        "    * Did I use histograms?\n",
        "    * Did I use some arguments to give further details regarding my plots?\n",
        "  * Missing Values\n",
        "    * Did I check if data had any missing values?\n",
        "    * If so, did I try to replace them with more appropriate values?\n",
        "  * Creating DataFrames\n",
        "    * Did I create a DataFrame using a list and/or dictionary?\n",
        "  * Reading and Writing CSVs\n",
        "    * Did I read a csv file when doing the applications? <br>\n",
        "    * Did I write to a csv file when doing the applications?"
      ]
    }
  ]
}